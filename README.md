# LanguageModeling_with_RNN

Text is a type of sequence data where the order of the words holds context and patterns that are relevant to the later words. Because the order of the data matters, the model must also take that order into consideration. Deep learning models such as recurrent neural networks and one-dimensional neural networks have an architecture that are able to capture the correlations of the sequences in the text. A type of recurrent neural networks is long short term memory models that, like traditional recurrent neural networks, is able to save the memory of the correlations. Long short term memory, however, separates memory into short and long term memory, which elevates the issue that simple recurrent neural networks loses long term memory. 

The goal is to build these three different model architectures (RNN, LSTM, 1-D CNN) to classify a subset of AG news articles into its corresponding topic labels. Not only does this problem require a model to be built but also to process the text into vectors to provide the text with meaningful representation as well as build a vocabulary with it. Please refer to the report on the model experiments.

Overall, the simple RNN, LSTM, and 1-D CNN models were all able to successfully classify the articles into its respective topic class. Experimenet C.5, the single layer unidirectional LSTM with vocabulary size of 2000, performed the best and should be implemented. The test accuracy is 0.88 which is very good, and based off the visualizations, this model does not overfit and does a good job discriminating between the four topic classes. Not only is the test accuracy score better than the other models, but it also had the fastest processing time other than for the 1-D CNN model. The LSTM is more efficient than the simple RNN architecture, despite having more parameters to train. 
